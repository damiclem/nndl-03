{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worden: vector encoded words based text generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from src.dataset import WarAndPeace, WordToVector, RandomCrop, ToTensor\n",
    "from src.dataset import split_train_test\n",
    "from scr.embeddings import Embeddings, Glove\n",
    "from src.network import Worden\n",
    "from src.network import grid_search\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import optim, nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CPU device\n",
    "cpu = torch.device('cpu')\n",
    "# Define best device (GPU if available, CPU otherwise)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset\n",
    "dataset = WarAndPeace('./data/war-and-peace-tolstoj.txt', split_how='words', min_len=10)\n",
    "# Show dataset length (number of sentences)\n",
    "print('Dataset length:', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 3 sentences\n",
    "for i in range(10):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize set of words in text\n",
    "words = set()\n",
    "# Go through each sentence in dataset\n",
    "for i in range(len(dataset)):\n",
    "    # Turn sentence (list of words) into set\n",
    "    sentence = set(dataset[i])\n",
    "    # Update words set\n",
    "    words |= sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some available words\n",
    "print(', '.join([*words][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding dimension\n",
    "embedding_dim = 50\n",
    "\n",
    "print('Embedding dimension is: {:d}'.format(embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from glove\n",
    "glove = Glove.from_file(\n",
    "    path='data/glove.6B/glove.6B.50d.txt',\n",
    "    words=words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 3 embeddings\n",
    "print('Glove embeddings:')\n",
    "# Loop through each word, vector tuple\n",
    "for i, (word, vector) in enumerate(glove.items()):\n",
    "    # Print word to vector\n",
    "    print('{0:s} ({1:d}):\\t{2:s} ...'.format(word, i + 1,  ' '.join([str(v) for v in vector[:10]])))\n",
    "    # Early stopping\n",
    "    if i >= 10: break\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean and variance distribution in glove embeddings\n",
    "Embeddings.plot_embeddings(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embeddings for unknown words by sampling from normal distirbution, using found parameters\n",
    "mean = 0.01  # Found mean is circa 0.0\n",
    "std = 0.61  # Found std. dev. is circa 0.6\n",
    "\n",
    "# Initialize randomly sampled embeddings\n",
    "embeddings = Embeddings.from_normal(mean, std, dim=embedding_dim, words=words)\n",
    "# Loop through each embedded word\n",
    "for word, vector in embeddings.items():\n",
    "    # Subsititute current entry with glove one, if available\n",
    "    embeddings[word] = glove.get(word, vector)\n",
    "\n",
    "# Get list of words\n",
    "words = [*embeddings.keys()]\n",
    "# Get vectors as float tensor\n",
    "vectors = torch.tensor([*embeddings.values()], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean and variance distribution in retrieved embeddings\n",
    "Embeddings.plot_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation for dataset\n",
    "dataset.transform = transforms.Compose([\n",
    "    WordToVector(words),\n",
    "    RandomCrop(7),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 3 sentences shapes\n",
    "for i in range(3):\n",
    "    print('Sentence nr {:d} has shape {:d}'.format(i+1, *dataset[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split initial dataset in train dataset and test dataset\n",
    "train_dataset, test_dataset = split_train_test(dataset, 0.9)\n",
    "# Make train dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "# Make test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameters evaluation through grid search\n",
    "params, train_losses, train_times, test_losses, test_times = grid_search(\n",
    "    # Set training and testing dataloaders\n",
    "    train=train_dataloader, test=test_dataloader,\n",
    "    # Define network architecture\n",
    "    net=Worden, net__embedding_dim=[embedding_dim], net__hidden_units=[512, 256], net__layers_num=[5, 4, 3], net__hidden_type=['GRU', 'LSTM'],\n",
    "    net__trained_embeddings=[vectors], net__freeze_embeddings=[False],\n",
    "    # Define optimizer\n",
    "    optim=[optim.Adam], optim__weight_decay=[5e-4, 5e-3, 5e-2],\n",
    "    # Define loss function\n",
    "    loss_fn=[nn.nn.NLLLoss],\n",
    "    # Define (low) number of epochs\n",
    "    num_epochs=10,\n",
    "    # Set network device (CPU or GPU)\n",
    "    device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
